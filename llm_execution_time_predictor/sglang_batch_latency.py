"""
Benchmark the latency of running a single static batch without a server.

This script does not launch a server and uses the low-level APIs.
It accepts server arguments (the same as launch_server.py) and benchmark arguments (e.g., batch size, input lengths).

# Usage (latency test)
## with dummy weights:
python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3-8B-Instruct --load-format dummy
## sweep through multiple data points and store (append) the results in a jsonl file:
python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3-8B-Instruct --batch 1 12 14 --input-len 256 512 --output-len 32 256 --run-name test_run
## run with profiling:
python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3-8B-Instruct --batch 1 12 14 --input-len 256 512 --profile
# Usage (correctness test):
python -m sglang.bench_one_batch --model-path TinyLlama/TinyLlama-1.1B-Chat-v0.4 --correct

## Reference output (of the correctness test above, can be gpu dependent):
input_ids=[[1, 450, 7483, 310, 3444, 338], [1, 450, 7483, 310, 278, 3303, 13187, 290, 338], [1, 20628, 338, 263, 6575, 1460, 2462, 322, 306, 763]]

prefill logits (first half): tensor([[-10.0312,  -9.5000,   0.8931,  ...,  -4.9414,  -3.2422,  -3.3633],
        [-10.0312,  -9.5000,   0.8931,  ...,  -4.9414,  -3.2422,  -3.3633],
        [ -9.1875, -10.2500,   2.7129,  ...,  -4.3359,  -4.0664,  -4.1328]],
       device='cuda:0')

prefill logits (final): tensor([[-8.3125, -7.1172,  3.3457,  ..., -4.9570, -4.1328, -3.4141],
        [-8.9141, -9.0156,  4.1445,  ..., -4.9922, -4.4961, -4.0781],
        [-9.6328, -9.0547,  4.0195,  ..., -5.3047, -4.7148, -4.4570]],
       device='cuda:0')

========== Prompt 0 ==========
<s> The capital of France is Paris.
The capital of the United States is Washington, D.C.


========== Prompt 1 ==========
<s> The capital of the United Kindom is London.
The capital of the United Kingdom is London.
The capital of the

========== Prompt 2 ==========
<s> Today is a sunny day and I like to go for a walk in the park.
I'm going to the park
"""

import argparse
import dataclasses
import itertools
import json
import logging
import multiprocessing
import os
import time
from typing import Tuple

import numpy as np
import torch
import torch.distributed as dist
from tqdm import tqdm
from sglang.srt.configs.model_config import ModelConfig
from sglang.srt.distributed.parallel_state import destroy_distributed_environment
from sglang.srt.entrypoints.engine import _set_envs_and_config
from sglang.srt.hf_transformers_utils import get_tokenizer
from sglang.srt.managers.schedule_batch import Req, ScheduleBatch
from sglang.srt.managers.scheduler import Scheduler
from sglang.srt.model_executor.forward_batch_info import ForwardBatch
from sglang.srt.model_executor.model_runner import ModelRunner
from sglang.srt.sampling.sampling_params import SamplingParams
from sglang.srt.server_args import PortArgs, ServerArgs
from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
from sglang.srt.utils import (
    configure_logger,
    get_bool_env_var,
    kill_process_tree,
    require_mlp_sync,
    require_mlp_tp_gather,
    set_gpu_proc_affinity,
    suppress_other_loggers,
)


def random_batch(n, mean_L, skew="none", max_total=64_000, rng=None):
    """
    #Note: Generated by O3
    Generate a list of `n` prompt lengths whose arithmetic mean is `mean_L`
    and whose skew matches one of three buckets:

        skew="none"   → CV² < 0.1          (uniform)
        skew="medium" → 0.1 ≤ CV² < 1      (mild skew)
        skew="heavy"  → CV² ≥ 1, target≈2½ (noticeable but not extreme)

    The total tokens are exactly n*mean_L unless that would exceed `max_total`,
    in which case the batch is uniformly scaled down.
    """
    rng = rng or np.random.default_rng()
    # Target CV² for each bucket
    cv2_target = {"none": 0.05,
                  "medium": 0.4,
                  "heavy": 1.5}[skew]      # ← adjust here to make heavier/lighter
    
    # Log-normal parameters that give desired mean & CV²
    sigma   = (cv2_target ** 0.5) * mean_L
    log_var = np.log1p((sigma / mean_L) ** 2)
    lens = rng.lognormal(np.log(mean_L) - 0.5 * log_var, np.sqrt(log_var), size=n)

    # Rescale so arithmetic mean is exactly mean_L
    lens *= (mean_L * n) / lens.sum()
    lens = np.maximum(lens, 1).astype(int).tolist()

    # Uniformly down-scale if we exceed max_total
    tot = sum(lens)
    if tot > max_total:
        lens = [max(1, int(L * max_total / tot)) for L in lens]

    return lens


@dataclasses.dataclass
class BenchArgs:
    run_name: str = "default"
    batch_size: Tuple[int] = (1,)
    input_len: Tuple[int] = (1024,)
    output_len: Tuple[int] = (16,)
    result_filename: str = "result.jsonl"
    correctness_test: bool = False
    # This is only used for correctness test
    cut_len: int = 4
    log_decode_step: int = 0
    profile: bool = False
    profile_filename_prefix: str = "profile"
    batch_composition: list = dataclasses.field(default_factory=list)
    run_prefill_profiling: bool = False
    run_decode_profiling: bool = False
    max_decode_token_length: int = None
    chunk_prefill: bool = False
    chunk_size: int = 512
    skew: str = "none"

    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser):
        parser.add_argument("--run-name", type=str, default=BenchArgs.run_name)
        parser.add_argument(
            "--batch-size", type=int, nargs="+", default=BenchArgs.batch_size
        )
        parser.add_argument(
            "--input-len", type=int, nargs="+", default=BenchArgs.input_len
        )
        parser.add_argument(
            "--output-len", type=int, nargs="+", default=BenchArgs.output_len
        )
        parser.add_argument(
            "--result-filename", type=str, default=BenchArgs.result_filename
        )
        parser.add_argument("--correctness-test", action="store_true")
        parser.add_argument("--cut-len", type=int, default=BenchArgs.cut_len)
        parser.add_argument(
            "--log-decode-step",
            type=int,
            default=BenchArgs.log_decode_step,
            help="Log decode latency by step, default is set to zero to disable.",
        )
        parser.add_argument(
            "--profile", action="store_true", help="Use Torch Profiler."
        )
        parser.add_argument(
            "--run-prefill-profiling", action="store_true", help="Run prefill profiling."
        )
        parser.add_argument(
            "--run-decode-profiling", action="store_true", help="Run decode profiling." 
        )
        parser.add_argument(
            "--max-decode-token-length", type=int, default=None, help="Maximum token length to consider for decode profiling."
        )
        parser.add_argument(
            "--profile-filename-prefix",
            type=str,
            default=BenchArgs.profile_filename_prefix,
            help="Prefix of the profiling file names. The full profiling result file(s) be "
            '"[profile_filename_prefix]_batch[batch_size]_input[input_len]_output[output_len].trace.json.gz"',
        )
        parser.add_argument(
            "--chunk-prefill", 
            action="store_true", 
            help="Split prefill across multiple requests"
        )
        parser.add_argument(
            "--chunk-size", 
            type=int, 
            default=BenchArgs.chunk_size,
            help="Size of each prefill chunk (default: 512)"
        )
        parser.add_argument(
            "--skew",
            type=str,
            choices=["none", "medium", "heavy"],
            default=BenchArgs.skew,
            help="Batch length distribution skew: none (uniform), medium (mild skew), heavy (noticeable skew)"
        )

    @classmethod
    def from_cli_args(cls, args: argparse.Namespace):
        # use the default value's type to cast the args into correct types.
        attrs = [(attr.name, type(attr.default)) for attr in dataclasses.fields(cls)]
        return cls(
            **{attr: attr_type(getattr(args, attr)) for attr, attr_type in attrs if hasattr(args, attr)}
        )


def load_model(server_args, port_args, tp_rank):
    suppress_other_loggers()
    rank_print = print if tp_rank == 0 else lambda *args, **kwargs: None

    model_config = ModelConfig.from_server_args(server_args)
    model_runner = ModelRunner(
        model_config=model_config,
        mem_fraction_static=server_args.mem_fraction_static,
        gpu_id=tp_rank,
        tp_rank=tp_rank,
        tp_size=server_args.tp_size,
        pp_rank=0,
        pp_size=server_args.pp_size,
        nccl_port=port_args.nccl_port,
        server_args=server_args,
    )
    rank_print(f"max_total_num_tokens={model_runner.max_total_num_tokens}")
    tokenizer = get_tokenizer(
        server_args.tokenizer_path,
        tokenizer_mode=server_args.tokenizer_mode,
        trust_remote_code=server_args.trust_remote_code,
    )
    if server_args.tp_size > 1:
        dist.barrier()
    return model_runner, tokenizer


def prepare_inputs_for_correctness_test(bench_args, tokenizer):
    prompts = [
        "The capital of France is",
        "The capital of the United Kindom is",
        "Today is a sunny day and I like",
    ]
    input_ids = [tokenizer.encode(p) for p in prompts]
    sampling_params = SamplingParams(
        temperature=0,
        max_new_tokens=BenchArgs.output_len,
    )

    reqs = []
    for i in range(len(prompts)):
        assert len(input_ids[i]) > bench_args.cut_len

        tmp_input_ids = input_ids[i][: bench_args.cut_len]
        req = Req(
            rid=i,
            origin_input_text=prompts[i],
            origin_input_ids=tmp_input_ids,
            sampling_params=sampling_params,
        )
        req.prefix_indices = []
        req.fill_ids = req.origin_input_ids
        req.extend_input_len = len(req.fill_ids) - len(req.prefix_indices)
        req.logprob_start_len = len(req.origin_input_ids) - 1
        reqs.append(req)

    return input_ids, reqs


def prepare_extend_inputs_for_correctness_test(
    bench_args, input_ids, reqs, model_runner
):
    for i in range(len(reqs)):
        req = reqs[i]
        req.fill_ids += input_ids[i][bench_args.cut_len :]
        req.prefix_indices = model_runner.req_to_token_pool.req_to_token[
            i, : bench_args.cut_len
        ]
        req.extend_input_len = len(req.fill_ids) - len(req.prefix_indices)
        req.logprob_start_len = len(req.origin_input_ids) - 1
    return reqs


def prepare_synthetic_inputs_for_latency_test(batch_size, input_len, input_len_list=None, skew="none"):
    """
    Prepare synthetic inputs for latency testing.
    
    Args:
        batch_size: Number of requests in the batch
        input_len: Default input length (used when input_len_list is None)
        input_len_list: Optional list of input lengths for each request in the batch.
                       If provided, batch_size must match len(input_len_list).
        skew: Distribution skew for input lengths ("none", "medium", "heavy")
    """
    if input_len_list is not None:
        input_ids = [
                np.random.randint(0, 10000, size=(item,), dtype=np.int32)
                for item in input_len_list
        ]
    else:
        if skew == "none":
            # Original behavior - generate random input_ids with same length
            input_ids = np.random.randint(0, 10000, (batch_size, input_len), dtype=np.int32)
        else:
            # Generate varied lengths using random_batch function
            lengths = random_batch(batch_size, input_len, skew=skew)
            print(lengths)
            input_ids = []
            for length in lengths:
                input_ids.append(np.random.randint(0, 10000, length, dtype=np.int32))
            # Convert to consistent format expected by the rest of the code
            max_len = max(len(ids) for ids in input_ids)
            padded_input_ids = []
            for ids in input_ids:
                # Pad shorter sequences to max length for consistent processing
                padded = np.pad(ids, (0, max_len - len(ids)), constant_values=0)
                padded_input_ids.append(padded)
            input_ids = padded_input_ids
    
    sampling_params = SamplingParams(
        temperature=0,
        max_new_tokens=BenchArgs.output_len,
    )

    reqs = []
    for i in range(batch_size):
        current_input_ids = list(input_ids[i])
        if len(current_input_ids) == 0:
            continue
        req = Req(
            rid=i,
            origin_input_text="",
            origin_input_ids=current_input_ids,
            sampling_params=sampling_params,
        )
        req.prefix_indices = []
        req.fill_ids = req.origin_input_ids
        req.extend_input_len = len(req.fill_ids) - len(req.prefix_indices)
        req.logprob_start_len = len(req.origin_input_ids) - 1
        reqs.append(req)
    return reqs


@torch.no_grad
def extend(reqs, model_runner, chunk_prefill=False, chunk_size=512):
    if not chunk_prefill:
        # Original single-request prefill behavior
        batch = ScheduleBatch.init_new(
            reqs=reqs,
            req_to_token_pool=model_runner.req_to_token_pool,
            token_to_kv_pool_allocator=model_runner.token_to_kv_pool_allocator,
            tree_cache=None,
            model_config=model_runner.model_config,
            enable_overlap=False,
            spec_algorithm=SpeculativeAlgorithm.NONE,
            enable_custom_logit_processor=False,
        )
        batch.prepare_for_extend()
        _maybe_prepare_mlp_sync_batch(batch, model_runner)
        model_worker_batch = batch.get_model_worker_batch()
        forward_batch = ForwardBatch.init_new(model_worker_batch, model_runner)
        logits_output, _ = model_runner.forward(forward_batch)
        next_token_ids = model_runner.sample(logits_output, forward_batch)
        return next_token_ids, logits_output.next_token_logits, batch, 1  # 1 prefill iteration
    else:
        # Chunked prefill behavior
        return extend_chunked(reqs, model_runner, chunk_size)


@torch.no_grad
def extend_chunked(reqs, model_runner, chunk_size):
    # Build a map of remaining tokens for each req
    remaining = {req: req.fill_ids.copy() for req in reqs}
    # Reset each req
    for req in reqs:
        req.fill_ids = []
        req.extend_input_len = 0

    total_iters = 0
    batch = None
    next_logits = None
    next_token_ids = None

    # Keep chunking until every req is exhausted
    while any(remaining.values()):
        batch_reqs = []
        budget = chunk_size

        # Fill this batch up to chunk_size tokens total
        for req in reqs:
            tokens = remaining[req]
            if not tokens or budget <= 0:
                continue

            take = min(len(tokens), budget)
            chunk = tokens[:take]
            remaining[req] = tokens[take:]

            # rebuild fill_ids + extend_input_len
            prefix = (
                req.prefix_indices.tolist()
                if isinstance(req.prefix_indices, torch.Tensor)
                else req.prefix_indices
            )
            req.fill_ids = prefix + chunk
            req.extend_input_len = take

            batch_reqs.append(req)
            budget -= take

        batch = ScheduleBatch.init_new(
            reqs=batch_reqs,
            req_to_token_pool=model_runner.req_to_token_pool,
            token_to_kv_pool_allocator=model_runner.token_to_kv_pool_allocator,
            tree_cache=None,
            model_config=model_runner.model_config,
            enable_overlap=False,
            spec_algorithm=SpeculativeAlgorithm.NONE,
            enable_custom_logit_processor=False,
        )
        batch.prepare_for_extend()
        _maybe_prepare_mlp_sync_batch(batch, model_runner)
        worker_batch = batch.get_model_worker_batch()
        forward_batch = ForwardBatch.init_new(worker_batch, model_runner)
        logits_output, _ = model_runner.forward(forward_batch)
        next_logits = logits_output.next_token_logits
        next_token_ids = next_logits.argmax(dim=-1)
        for req in batch_reqs:
            req.prefix_indices = model_runner.req_to_token_pool.req_to_token[
                req.rid, :len(req.fill_ids)
            ]
        total_iters += 1

    return next_token_ids, next_logits, batch, total_iters

@torch.no_grad
def decode(input_token_ids, batch, model_runner):
    batch.output_ids = input_token_ids
    batch.prepare_for_decode()
    _maybe_prepare_mlp_sync_batch(batch, model_runner)
    model_worker_batch = batch.get_model_worker_batch()
    forward_batch = ForwardBatch.init_new(model_worker_batch, model_runner)
    logits_output, _ = model_runner.forward(forward_batch)
    next_token_ids = model_runner.sample(logits_output, forward_batch)
    return next_token_ids, logits_output.next_token_logits


def _maybe_prepare_mlp_sync_batch(batch: ScheduleBatch, model_runner):
    if require_mlp_sync(model_runner.server_args):
        Scheduler.prepare_mlp_sync_batch_raw(
            batch,
            dp_size=model_runner.server_args.dp_size,
            attn_tp_size=1,
            tp_group=model_runner.tp_group,
            get_idle_batch=None,
            disable_cuda_graph=model_runner.server_args.disable_cuda_graph,
            spec_algorithm=SpeculativeAlgorithm.NONE,
            speculative_num_draft_tokens=None,
            require_mlp_tp_gather=require_mlp_tp_gather(model_runner.server_args),
            disable_overlap_schedule=model_runner.server_args.disable_overlap_schedule,
        )


def correctness_test(
    server_args,
    port_args,
    bench_args,
    tp_rank,
):
    # Configure the logger
    configure_logger(server_args, prefix=f" TP{tp_rank}")
    rank_print = print if tp_rank == 0 else lambda *args, **kwargs: None

    # Load the model
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)

    # Prepare inputs
    input_ids, reqs = prepare_inputs_for_correctness_test(bench_args, tokenizer)
    rank_print(f"\n{input_ids=}\n")

    if bench_args.cut_len > 0:
        # Prefill
        next_token_ids, next_token_logits, batch, prefill_iterations = extend(
            reqs, model_runner, bench_args.chunk_prefill, bench_args.chunk_size
        )
        rank_print(f"prefill logits (first half): {next_token_logits} \n")

    # Prepare extend inputs
    reqs = prepare_extend_inputs_for_correctness_test(
        bench_args, input_ids, reqs, model_runner
    )

    # Extend (prefill w/ KV cache)
    next_token_ids, next_token_logits, batch, prefill_iterations = extend(
        reqs, model_runner, bench_args.chunk_prefill, bench_args.chunk_size
    )
    rank_print(f"prefill logits (final): {next_token_logits} \n")

    # Decode
    output_ids = [input_ids[i] + [next_token_ids[i]] for i in range(len(input_ids))]
    for _ in range(bench_args.output_len[0] - 1):
        next_token_ids, _ = decode(next_token_ids, batch, model_runner)
        next_token_ids_list = next_token_ids.tolist()
        for i in range(len(reqs)):
            output_ids[i].append(next_token_ids_list[i])

    # Print output texts
    for i in range(len(reqs)):
        rank_print(f"========== Prompt {i} ==========")
        rank_print(tokenizer.decode(output_ids[i]), "\n")


def synchronize(device):
    torch.get_device_module(device).synchronize()

def generate_distribution_of_skewed_batch(total_elements, batch_size, skew):
    if batch_size == 1:
        return [total_elements]

    if skew == 0:
        counts = np.ones(batch_size)
    else:
        ranks = np.arange(1, batch_size + 1)
        counts = 1/ranks ** skew

    counts *= total_elements / np.sum(counts)     
    counts_rounded = np.floor(counts).astype(int)  
    remainder = total_elements - np.sum(counts_rounded) 
    if remainder > 0:
        # Adds the remainder to the top `remainder` elements
        frac_parts = counts - counts_rounded
        top_indices = np.argpartition(-frac_parts, remainder)[:remainder]
        counts_rounded[top_indices] += 1
    counts_rounded = [int(x) for x in counts_rounded]
    return counts_rounded

def run_prefill_config(model_runner, skewed_batch_lens, clear_cache=True, reqs=None):
    """
    Run the prefill with a specific skewed batch configuration.
    This function is used to profile the prefill performance with different skew configurations.
    """
    if clear_cache:
        model_runner.req_to_token_pool.clear()
        model_runner.token_to_kv_pool_allocator.clear()
    if reqs is None:
        reqs = prepare_synthetic_inputs_for_latency_test(
            len(skewed_batch_lens), max(skewed_batch_lens), skewed_batch_lens
        )
    synchronize(model_runner.device)
    start_time = time.perf_counter()
    next_token_ids, _, batch = extend(reqs, model_runner)
    synchronize(model_runner.device)

    end_time = time.perf_counter()
    latency = end_time - start_time
    if not clear_cache:
        for req in reqs:
            if len(req.fill_ids) != 0:
                req.fill_ids = req.fill_ids[:req.extend_input_len]
                req.prefix_indices = model_runner.req_to_token_pool.req_to_token[
                    req.rid, :req.extend_input_len
                ]
                req.logprob_start_len = len(req.origin_input_ids) - 1
    
    total_tokens = sum(skewed_batch_lens)
    throughput = total_tokens / latency
    return latency, throughput, (next_token_ids, batch)

def run_prefill_in_chunks_to_load_cache(model_runner, skewed_batch_lens, reqs, chunk_size=16384):
    """
    Run the prefill in chunks of up to chunk_size tokens to load the KV cache.
    This simulates a more realistic scenario where the KV cache is filled in chunks.
    """
    start_index = 0
    all_batches = []
    all_next_token_ids = []

    while start_index < len(skewed_batch_lens):
        current_sum = 0
        end_index = start_index

        while end_index < len(skewed_batch_lens) and current_sum + skewed_batch_lens[end_index] <= chunk_size:
            current_sum += skewed_batch_lens[end_index]
            end_index += 1

        if end_index == start_index:
            end_index += 1

        current_chunk = skewed_batch_lens[start_index:end_index]
        current_reqs = reqs[start_index:end_index]

        latency, throughput, (next_token_ids, batch) = run_prefill_config(
            model_runner,
            current_chunk,
            clear_cache=False,
            reqs=current_reqs
        )
        all_batches.append(batch)
        all_next_token_ids.extend(next_token_ids)

        start_index = end_index  # advance to next chunk

    batch0: ScheduleBatch = all_batches[0]
    for batch in all_batches[1:]:
        batch0.merge_batch(batch)

    return batch0, all_next_token_ids

def warmup_model(model_runner):
    np.random.seed(42)
    warmup_reqs = prepare_synthetic_inputs_for_latency_test(
        batch_size=1, input_len=1024, input_len_list=[1024]
    )
    synchronize(model_runner.device)
    (next_token_ids,_, batch) = extend(warmup_reqs, model_runner)
    synchronize(model_runner.device)
    
    decode(
        torch.tensor(next_token_ids, device=model_runner.device),
        batch,
        model_runner,
    )
    synchronize(model_runner.device)


def get_rank_print(model_runner):
    return print if model_runner.tp_rank == 0 else lambda *args, **kwargs: None

def write_results_to_file(results, filename):
    with open(filename, "w") as fout:
        for result in results:
            fout.write(json.dumps(result) + "\n")

def filter_token_lengths(lengths, max_length):
    return [x for x in lengths if x <= max_length]

def generate_test_configs(batch_sizes, token_lengths, skew_values):
    return [
        (batch_size, token_len, skew)
        for batch_size in batch_sizes
        for token_len in token_lengths
        for skew in skew_values
    ]

def run_prefill_config_with_skew(model_runner, batch_size, token_len, skew, reqs=None):
    skewed_batch_lens = generate_distribution_of_skewed_batch(token_len, batch_size, skew)
    num_non_empty = sum(1 for x in skewed_batch_lens if x > 0)
    latency, throughput, data = run_prefill_config(model_runner, skewed_batch_lens, reqs=reqs)
    return {
        "prefill_batch_size": num_non_empty,
        "prefill_token_length": token_len,
        "skew": skew,
        "latency": latency,
        "throughput": throughput,
        "skewed_batch_lens": skewed_batch_lens,
    }, data

def run_prefill_profiling(model_runner, max_prefill_batch_size, profile_filename_prefix="profile"):
    warmup_model(model_runner)
    rank_print = get_rank_print(model_runner)
    # rank_print("Warmup done.")
    print("Warmup done.")
    rank_print(f"Running prefill profiling with max_prefill_batch_size={max_prefill_batch_size}")

    prefill_batch_sizes = [1, 2, 4, 8, 16, 32, 48, 64, 72, 84, 128, 256]
    token_lengths = filter_token_lengths(
        [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 10240, 16384],
        max_prefill_batch_size,
    )
    skews = [0, 0.5, 1.0, 1.5]
    test_configs = generate_test_configs(prefill_batch_sizes, token_lengths, skews)

    prefill_results = []
    tqdm_bar = tqdm(total=len(test_configs), desc="Prefill Profiling Progress")

    for batch_size, token_len, skew in test_configs:
        result, _ = run_prefill_config_with_skew(model_runner, batch_size, token_len, skew)
        prefill_results.append(result)
        tqdm_bar.update(1)

    write_results_to_file(prefill_results, f"prefill_{profile_filename_prefix}_profiling_tp{model_runner.tp_rank}.jsonl")
    return prefill_results

def run_decoding_config(model_runner, next_input_ids, batch):
    """
    Run the decoding with a specific skewed batch configuration.
    This function is used to profile the decoding performance with different skew configurations.
    """
    synchronize(model_runner.device)
    start_time = time.perf_counter()
    next_token_ids, _ = decode(next_input_ids, batch, model_runner)
    synchronize(model_runner.device)
    end_time = time.perf_counter()
    latency = end_time - start_time
    total_tokens = len(batch.reqs)
    throughput = total_tokens / latency
    return latency, throughput

def run_decode_profiling(model_runner, max_token_length=None, profile_filename_prefix="profile"):
    warmup_model(model_runner)
    rank_print = get_rank_print(model_runner)
    rank_print("Warmup done.")

    prefill_batch_sizes = [1, 2, 4, 8, 16, 32, 48, 64, 72, 84, 128]
    token_lengths = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 10240, 16384, 20480, 32768, 40960]
    max_tokens_limit = int(model_runner.max_total_num_tokens * 0.8)
    if max_token_length is not None:
        max_tokens_limit = min(max_tokens_limit, max_token_length)
    token_lengths = sorted(filter_token_lengths(token_lengths, max_tokens_limit))
    skews = [0, 0.5, 1.0, 1.5, 2.0]
    test_configs = generate_test_configs(prefill_batch_sizes, token_lengths, skews)
    decode_results = []
    tqdm_bar = tqdm(total=len(test_configs), desc="Decode Profiling Progress")

    for batch_size, token_len, skew in test_configs:
        skewed_batch_lens = generate_distribution_of_skewed_batch(token_len, batch_size, skew)
        num_non_empty = sum(1 for x in skewed_batch_lens if x > 0)
        reqs = prepare_synthetic_inputs_for_latency_test(
            batch_size=num_non_empty, input_len=token_len, input_len_list=skewed_batch_lens
        )
        batch, next_token_ids = run_prefill_in_chunks_to_load_cache(
            model_runner, skewed_batch_lens, chunk_size=16384, reqs=reqs
        )
        latency, throughput = run_decoding_config(model_runner, torch.tensor(next_token_ids, device=model_runner.device), batch)

        decode_results.append({
            "prefill_batch_size": num_non_empty,
            "prefill_token_length": token_len,
            "skew": skew,
            "latency": latency,
            "throughput": throughput,
            "skewed_batch_lens": skewed_batch_lens,
        })

        model_runner.req_to_token_pool.clear()
        model_runner.token_to_kv_pool_allocator.clear()
        tqdm_bar.update(1)

    write_results_to_file(decode_results, f"{profile_filename_prefix}_decode_profiling_tp{model_runner.tp_rank}.jsonl")
    return decode_results

def latency_test_run_once(
    run_name,
    model_runner,
    rank_print,
    reqs,
    batch_size,
    input_len,
    device,
    log_decode_step,
    profile,
    profile_filename_prefix,
    chunk_prefill=False,
    chunk_size=512,
    skew="none",
):
    model_config = model_runner.model_config
    
    max_batch_size = model_runner.max_total_num_tokens // (input_len + 1)
    if batch_size > max_batch_size:
        rank_print(
            f"skipping ({batch_size}, {input_len}, {1}) due to max batch size limit"
        )
        return

    # Clear the pools.
    model_runner.req_to_token_pool.clear()
    model_runner.token_to_kv_pool_allocator.clear()

    measurement_results = {
        "run_name": run_name,
        "batch_size": batch_size,
        "input_len": input_len,
        "output_len": 1,
    }

    tot_latency = 0

    profiler = None
    if profile:
        profiler = torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            with_stack=True,
        )
        profiler.start()

    # Prefill
    synchronize(device)
    tic = time.perf_counter()
    next_token_ids, _, batch, prefill_iterations = extend(reqs, model_runner, chunk_prefill, chunk_size)
    synchronize(device)
    prefill_latency = time.perf_counter() - tic
    tot_latency += prefill_latency
    throughput = input_len * batch_size / prefill_latency
    rank_print(
        f"Prefill. latency: {prefill_latency:6.5f} s, throughput: {throughput:9.2f} token/s, iterations: {prefill_iterations}"
    )
    measurement_results["prefill_latency"] = prefill_latency
    measurement_results["prefill_throughput"] = throughput
    measurement_results["prefill_iterations"] = prefill_iterations

    # Decode
    decode_latencies = []
    for i in range(1):
        synchronize(device)
        tic = time.perf_counter()
        next_token_ids, _ = decode(next_token_ids, batch, model_runner)
        synchronize(device)
        latency = time.perf_counter() - tic
        tot_latency += latency
        throughput = batch_size / latency
        decode_latencies.append(latency)
        if i < 5 or (log_decode_step > 0 and i % log_decode_step == 0):
            rank_print(
                f"Decode {i}. Batch size: {batch_size}, latency: {latency:6.5f} s, throughput: {throughput:9.2f} token/s"
            )

    if profile:
        profiler.stop()
        profile_filename = f"{profile_filename_prefix}_batch{batch_size}_input{input_len}_output{output_len}.trace.json.gz"
        parent_dir = os.path.dirname(os.path.abspath(profile_filename))
        os.makedirs(parent_dir, exist_ok=True)
        profiler.export_chrome_trace(profile_filename)
        rank_print(f"torch profiler chrome trace saved to {profile_filename}")
    output_len = 1
    # Record decode timing from 2nd output
    if output_len >= 1:
        med_decode_latency = np.median(decode_latencies)
        med_decode_throughput = batch_size / med_decode_latency
        rank_print(
            f"Decode.  median latency: {med_decode_latency:6.5f} s, median throughput: {med_decode_throughput:9.2f} token/s"
        )
        measurement_results["median_decode_latency"] = med_decode_latency
        measurement_results["median_decode_throughput"] = med_decode_throughput

    throughput = (input_len + output_len) * batch_size / tot_latency
    rank_print(
        f"Total. latency: {tot_latency:6.3f} s, throughput: {throughput:9.2f} token/s"
    )
    measurement_results["total_latency"] = tot_latency
    measurement_results["overall_throughput"] = throughput
    measurement_results["skew"] = skew
    return measurement_results

def prefill_latency_test(server_args, port_args, bench_args, tp_rank):
    # Set CPU affinity
    if get_bool_env_var("SGLANG_SET_CPU_AFFINITY"):
        set_gpu_proc_affinity(server_args.tp_size, server_args.nnodes, tp_rank)

    # Configure the logger
    configure_logger(server_args, prefix=f" TP{tp_rank}")
    rank_print = print if tp_rank == 0 else lambda *args, **kwargs: None

    # Load the model
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)

    if bench_args.run_prefill_profiling:
        run_prefill_profiling(model_runner, bench_args.input_len[0], bench_args.profile_filename_prefix)
    
    if server_args.tp_size > 1:
        destroy_distributed_environment()

def decode_latency_test(server_args, port_args, bench_args: BenchArgs, tp_rank):
    # Set CPU affinity
    if get_bool_env_var("SGLANG_SET_CPU_AFFINITY"):
        set_gpu_proc_affinity(server_args.tp_size, server_args.nnodes, tp_rank)
    
    # Configure the logger
    configure_logger(server_args, prefix=f" TP{tp_rank}")
    rank_print = print if tp_rank == 0 else lambda *args, **kwargs: None

    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
    if bench_args.run_decode_profiling:
        run_decode_profiling(model_runner, bench_args.max_decode_token_length, bench_args.profile_filename_prefix)
    
    if server_args.tp_size > 1:
        destroy_distributed_environment()


def latency_test(
    server_args,
    port_args,
    bench_args,
    tp_rank,
    disable_rank_print=True
):
    # Set CPU affinity
    if get_bool_env_var("SGLANG_SET_CPU_AFFINITY"):
        set_gpu_proc_affinity(server_args.tp_size, server_args.nnodes, tp_rank)

    # Configure the logger
    configure_logger(server_args, prefix=f" TP{tp_rank}")
    if disable_rank_print:
        rank_print = lambda *args, **kwargs: None
    else:
        rank_print = print if tp_rank == 0 else lambda *args, **kwargs: None

    # Load the model
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)

    # Prepare inputs for warm up
    reqs = prepare_synthetic_inputs_for_latency_test(
        bench_args.batch_size[0], bench_args.input_len[0], skew='none'
    )

    # Warm up
    rank_print("Warmup ...")
    latency_test_run_once(
        bench_args.run_name,
        model_runner,
        rank_print,
        reqs,
        bench_args.batch_size[0],
        bench_args.input_len[0],
        server_args.device,
        log_decode_step=0,
        profile=False,
        profile_filename_prefix="",  # not used
        chunk_prefill=bench_args.chunk_prefill,
        chunk_size=bench_args.chunk_size,
    )

    rank_print("Benchmark ...")

    # Run the sweep
    result_list = []
    for bs, il, ol in itertools.product(
        bench_args.batch_size, bench_args.input_len, bench_args.output_len
    ):
        reqs = prepare_synthetic_inputs_for_latency_test(bs, il, skew=bench_args.skew)
        ret = latency_test_run_once(
            bench_args.run_name,
            model_runner,
            rank_print,
            reqs,
            bs,
            il,
            server_args.device,
            bench_args.log_decode_step,
            bench_args.profile if tp_rank == 0 else None,
            bench_args.profile_filename_prefix,
            chunk_prefill=bench_args.chunk_prefill,
            chunk_size=bench_args.chunk_size, 
            skew=bench_args.skew
        )
        if ret is not None:
            result_list.append(ret)

    # Write results in jsonlines format on rank 0.
    if tp_rank == 0 and bench_args.result_filename:
        with open(bench_args.result_filename, "a") as fout:
            for result in result_list:
                fout.write(json.dumps(result) + "\n")

    if server_args.tp_size > 1:
        destroy_distributed_environment()


def main(server_args, bench_args):
    server_args.cuda_graph_max_bs = max(bench_args.batch_size)

    _set_envs_and_config(server_args)

    if server_args.model_path:
        if bench_args.correctness_test:
            work_func = correctness_test
        elif bench_args.run_prefill_profiling:
            work_func = prefill_latency_test
        elif bench_args.run_decode_profiling:
            work_func = decode_latency_test
        else:
            work_func = latency_test

    else:
        raise ValueError(
            "Provide --model-path for running the tests or "
            "provide --result-filename for plotting the results"
        )

    port_args = PortArgs.init_new(server_args)

    if server_args.tp_size == 1:
        work_func(server_args, port_args, bench_args, 0)
    else:
        workers = []
        for tp_rank in range(server_args.tp_size):
            proc = multiprocessing.Process(
                target=work_func,
                args=(
                    server_args,
                    port_args,
                    bench_args,
                    tp_rank,
                ),
            )
            proc.start()
            workers.append(proc)

        for proc in workers:
            proc.join()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    ServerArgs.add_cli_args(parser)
    BenchArgs.add_cli_args(parser)
    args = parser.parse_args()
    server_args = ServerArgs.from_cli_args(args)
    bench_args = BenchArgs.from_cli_args(args)

    logging.basicConfig(
        level=getattr(logging, server_args.log_level.upper()),
        format="%(message)s",
    )

    try:
        main(server_args, bench_args)
    finally:
        if server_args.tp_size != 1:
            kill_process_tree(os.getpid(), include_parent=False)
